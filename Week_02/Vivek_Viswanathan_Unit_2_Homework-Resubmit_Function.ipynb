{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise, we will start with the same dataset as in the previous assignment. As an extension to our previous assignment, we will: \n",
    "\n",
    "- Create a method for normalizing (Scaling) the vocabulary size of 3 texts\n",
    "- Create a method for scoring the long-word vocabulary size of a text\n",
    "    - We will build an array of all words greater than 10 characters\n",
    "    - Normalize / Scale the score as in previous step\n",
    "    - Store it in the results dataframe\n",
    "- Create a Text Difficulty Score\n",
    "    - Combine the Lexical Diversity Score, Normalized Vocabulary Size Score and the Normalized long-word vocabulary score\n",
    "    - Calculate the mean\n",
    "- Compare the new score between the 3 texts and explain the observation in the conclusion section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional text processing compared to Assignment 1:\n",
    "\n",
    "- We have identified the starting and end of the text to filter out table of contents, preface, table of figures, etc. \n",
    "- We have converted all words to lowercase so that we don't count the same word twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the text files\n",
    "- Download the files using the .txt URL.\n",
    "- Decode (UTF-8 text) \n",
    "- Identify the starting and ending position of the text and strip out the unnecessary text\n",
    "- Tokenize the words\n",
    "- Convert text to lower case\n",
    "- Remove stop words\n",
    "- Remove punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, start, end):\n",
    "    text_sub = text[start:end] # Substring from Start to End of actual Text\n",
    "    text_tokens = word_tokenize(text_sub) # Tokenize the words\n",
    "    text_lower = [w.lower() for w in text_tokens] # Convert to lower case\n",
    "    text_rem_stop = [word for word in text_lower if not word in stop_words] # remove stop words\n",
    "    text_rem_punc = [word for word in text_rem_stop if word.isalpha()] # remove punctuations\n",
    "    return(text_rem_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_grade3_text = \"http://www.gutenberg.org/cache/epub/14766/pg14766.txt\"\n",
    "grade3_text_url_open = request.urlopen(url_grade3_text)\n",
    "text_grade3_raw = grade3_text_url_open.read().decode('utf-8-sig')\n",
    "text_grade3_raw_start = text_grade3_raw.find(\"MCGUFFEY\\'S\\r\\n\\r\\nTHIRD READER\")\n",
    "text_grade3_raw_end = text_grade3_raw.rfind(\"End of the Project Gutenberg EBook of McGuffey\\'s Third Eclectic Reader\")\n",
    "text_grade3_tokenize = clean_text(text_grade3_raw, text_grade3_raw_start, text_grade3_raw_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_grade4_text = \"http://www.gutenberg.org/cache/epub/14880/pg14880.txt\"\n",
    "grade4_text_url_open = request.urlopen(url_grade4_text)\n",
    "text_grade4_raw = grade4_text_url_open.read().decode('utf-8-sig')\n",
    "text_grade4_raw_start = text_grade4_raw.find(\"MCGUFFEY\\'S FOURTH READER\")\n",
    "text_grade4_raw_end = text_grade4_raw.rfind(\"End of the Project Gutenberg EBook of McGuffey\\'s Fourth Eclectic Reader\")\n",
    "text_grade4_tokenize = clean_text(text_grade4_raw, text_grade4_raw_start, text_grade4_raw_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_grade5_text = \"http://www.gutenberg.org/cache/epub/15040/pg15040.txt\"\n",
    "grade5_text_url_open = request.urlopen(url_grade5_text)\n",
    "text_grade5_raw = grade5_text_url_open.read().decode('utf-8-sig')\n",
    "text_grade5_raw_start = text_grade5_raw.find(\"McGuffey\\'s Fifth Reader\")\n",
    "text_grade5_raw_end = text_grade5_raw.rfind(\"End of the Project Gutenberg EBook of McGuffey\\'s Fifth Eclectic Reader\")\n",
    "text_grade5_tokenize = clean_text(text_grade5_raw, text_grade5_raw_start, text_grade5_raw_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27495\n",
      "6316\n"
     ]
    }
   ],
   "source": [
    "print(len(text_grade4_tokenize))\n",
    "print(len(set(text_grade4_tokenize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and store the Lexical Diversity score in a dataframe\n",
    "\n",
    "- Define a blank dataframe\n",
    "- Define a function to calculate the lexical diversity score\n",
    "- Call the function for the text from different grades and store the corresponding values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Grade_Level, Lexical_Diversity_Score, Vocabulary_Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Grade_Level'\n",
    "                                   , 'Lexical_Diversity_Score'\n",
    "                                   , 'Vocabulary_Count'\n",
    "                                  ])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    vocab_cnt = len(set(text))\n",
    "    lex_div = vocab_cnt / len(text)\n",
    "    return (lex_div, vocab_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lex_div, vocab_cnt) = lexical_diversity(text_grade3_tokenize)\n",
    "\n",
    "results_df = results_df.append({'Grade_Level': 'Grade 3'\n",
    "                                , 'Lexical_Diversity_Score': lex_div\n",
    "                                , 'Vocabulary_Count': vocab_cnt}\n",
    "                              , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lex_div, vocab_cnt) = lexical_diversity(text_grade4_tokenize)\n",
    "\n",
    "results_df = results_df.append({'Grade_Level': 'Grade 4'\n",
    "                                , 'Lexical_Diversity_Score': lex_div\n",
    "                                , 'Vocabulary_Count': vocab_cnt}\n",
    "                              , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lex_div, vocab_cnt) = lexical_diversity(text_grade5_tokenize)\n",
    "\n",
    "results_df = results_df.append({'Grade_Level': 'Grade 5'\n",
    "                                , 'Lexical_Diversity_Score': lex_div\n",
    "                                , 'Vocabulary_Count': vocab_cnt}\n",
    "                              , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score\n",
       "0     Grade 3                 0.246825\n",
       "1     Grade 4                 0.229714\n",
       "2     Grade 5                 0.229137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.iloc[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>2974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>9986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level Vocabulary_Count\n",
       "0     Grade 3             2974\n",
       "1     Grade 4             6316\n",
       "2     Grade 5             9986"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.iloc[:, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>2974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "      <td>6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>9986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score Vocabulary_Count\n",
       "0     Grade 3                 0.246825             2974\n",
       "1     Grade 4                 0.229714             6316\n",
       "2     Grade 5                 0.229137             9986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Scores\n",
    "\n",
    "We need scale and translate the feature such that it is between zero and one.\n",
    "\n",
    "- Use MinMaxScaler from sklearn\n",
    "- Scale the Vocabulary Size of the text from each grade between 0 and 1\n",
    "- Store the results in the same dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_normalized = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using the formula \n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) /\n",
    "                                                    (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = n_vocab_size(text_grade3_tokenize,\n",
    "                          text_grade4_tokenize,\n",
    "                          text_grade5_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "      <th>Vocab_Cnt_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>2974</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "      <td>6316</td>\n",
       "      <td>0.476612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>9986</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score Vocabulary_Count  Vocab_Cnt_Scaled\n",
       "0     Grade 3                 0.246825             2974          0.000000\n",
       "1     Grade 4                 0.229714             6316          0.476612\n",
       "2     Grade 5                 0.229137             9986          1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Vocab_Cnt_Scaled'] = vocab_size[2]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "      <th>Vocab_Cnt_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>2974</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "      <td>6316</td>\n",
       "      <td>0.476612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>9986</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score Vocabulary_Count  Vocab_Cnt_Scaled\n",
       "0     Grade 3                 0.246825             2974          0.000000\n",
       "1     Grade 4                 0.229714             6316          0.476612\n",
       "2     Grade 5                 0.229137             9986          1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scaled vocabulary size score increases with grade level. This is a better indicator than the lexical diversity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Long Words\n",
    "\n",
    "- Use nltk.FreqDist to calculate the occurrence of each word within the text.\n",
    "- Make a list of all words greater than 10 characters in each of the texts.\n",
    "- Store the counts of complex words in the results dataframe.\n",
    "- Scale the complex word count between 0 and 1.\n",
    "- Store the results in the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackberries -> 1\n",
      "commandment -> 2\n",
      "commandments -> 2\n",
      "constructed -> 1\n",
      "deliverance -> 1\n",
      "discouraged -> 1\n",
      "disobedient -> 1\n",
      "encountered -> 1\n",
      "experienced -> 1\n",
      "forgiveness -> 1\n",
      "gingerbread -> 2\n",
      "grandfather -> 1\n",
      "grandmother -> 12\n",
      "immediately -> 2\n",
      "neighborhood -> 1\n",
      "newfoundland -> 1\n",
      "overflowing -> 1\n",
      "pocketknife -> 1\n",
      "satisfaction -> 2\n",
      "schoolhouse -> 2\n",
      "schoolmates -> 2\n",
      "sorrowfully -> 1\n",
      "strawberries -> 4\n",
      "transgressions -> 1\n"
     ]
    }
   ],
   "source": [
    "fdist_grade3 = nltk.FreqDist(text_grade3_tokenize)\n",
    "\n",
    "grade3_complex_words = []\n",
    "for word in sorted(fdist_grade3):\n",
    "    if(len(word) > 10):\n",
    "        print(word, '->', fdist_grade3[word], end='\\n')\n",
    "        grade3_complex_words.append(word)\n",
    "        \n",
    "results_df.loc[0, 'Complex_Wrd_Cnt'] = len(grade3_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbreviation -> 1\n",
      "aberbrothok -> 4\n",
      "accidentally -> 2\n",
      "accompanied -> 2\n",
      "accomplishing -> 1\n",
      "accordingly -> 2\n",
      "acquaintance -> 1\n",
      "advancement -> 1\n",
      "affectionate -> 1\n",
      "application -> 1\n",
      "appreciated -> 1\n",
      "approaching -> 2\n",
      "approbation -> 1\n",
      "appropriate -> 1\n",
      "arrangements -> 1\n",
      "ascertained -> 1\n",
      "astonishment -> 2\n",
      "beautifully -> 2\n",
      "capabilities -> 1\n",
      "chanticleer -> 1\n",
      "cheerfulness -> 1\n",
      "christopher -> 1\n",
      "churchgoing -> 1\n",
      "circumstance -> 2\n",
      "circumstances -> 1\n",
      "comfortable -> 2\n",
      "comfortably -> 2\n",
      "comfortless -> 1\n",
      "commandment -> 2\n",
      "commandments -> 2\n",
      "commencement -> 1\n",
      "communication -> 2\n",
      "comparatively -> 2\n",
      "competition -> 2\n",
      "complaining -> 2\n",
      "composition -> 19\n",
      "compositions -> 1\n",
      "concentration -> 1\n",
      "confidingly -> 1\n",
      "confinement -> 1\n",
      "conjectured -> 1\n",
      "conscientious -> 1\n",
      "consciousness -> 1\n",
      "consequence -> 3\n",
      "consequences -> 2\n",
      "consideration -> 1\n",
      "considerest -> 1\n",
      "considering -> 1\n",
      "consolation -> 1\n",
      "consultation -> 1\n",
      "contemptible -> 1\n",
      "contentedly -> 1\n",
      "continually -> 3\n",
      "continuance -> 1\n",
      "contrivance -> 1\n",
      "conversation -> 2\n",
      "countenance -> 7\n",
      "countenances -> 1\n",
      "counterfeit -> 1\n",
      "courageously -> 1\n",
      "cultivating -> 1\n",
      "cultivation -> 2\n",
      "daffydowndilly -> 1\n",
      "declaration -> 1\n",
      "definitions -> 1\n",
      "deliverance -> 1\n",
      "descendants -> 1\n",
      "description -> 3\n",
      "desperately -> 4\n",
      "despitefully -> 1\n",
      "destruction -> 3\n",
      "dexterously -> 1\n",
      "disagreeable -> 4\n",
      "disappeared -> 5\n",
      "disappointed -> 1\n",
      "disappointments -> 1\n",
      "discernment -> 1\n",
      "disconcerted -> 2\n",
      "disconsolate -> 2\n",
      "discouraged -> 4\n",
      "discouragements -> 1\n",
      "disgraceful -> 2\n",
      "disposition -> 6\n",
      "distinction -> 1\n",
      "distinctive -> 1\n",
      "distinguish -> 1\n",
      "distinguished -> 2\n",
      "distribution -> 1\n",
      "diversified -> 1\n",
      "earthenware -> 1\n",
      "effectually -> 1\n",
      "enchantment -> 2\n",
      "encompassed -> 1\n",
      "encouraging -> 1\n",
      "endeavoring -> 1\n",
      "enlightened -> 1\n",
      "entertained -> 1\n",
      "established -> 1\n",
      "establishment -> 1\n",
      "evaporation -> 1\n",
      "everlasting -> 1\n",
      "exaggerated -> 1\n",
      "examination -> 2\n",
      "exclamations -> 1\n",
      "exhibitions -> 2\n",
      "expectation -> 1\n",
      "experiments -> 2\n",
      "expressions -> 1\n",
      "extraordinary -> 2\n",
      "festivities -> 1\n",
      "flourishing -> 1\n",
      "forgiveness -> 2\n",
      "fortification -> 1\n",
      "fortunately -> 1\n",
      "foundations -> 1\n",
      "furthermore -> 1\n",
      "gingerbread -> 1\n",
      "grandfather -> 1\n",
      "grandmother -> 3\n",
      "handkerchief -> 1\n",
      "helplessness -> 1\n",
      "illustrating -> 1\n",
      "illustration -> 1\n",
      "illustrious -> 2\n",
      "immediately -> 11\n",
      "impertinent -> 1\n",
      "impetuosity -> 1\n",
      "impracticable -> 1\n",
      "improvement -> 4\n",
      "independence -> 1\n",
      "independent -> 1\n",
      "indignation -> 3\n",
      "industrious -> 4\n",
      "ingratitude -> 1\n",
      "inhabitants -> 7\n",
      "innumerable -> 1\n",
      "inscription -> 3\n",
      "insensibility -> 1\n",
      "instruction -> 4\n",
      "intelligent -> 1\n",
      "intercourse -> 1\n",
      "interesting -> 6\n",
      "interfering -> 1\n",
      "interminable -> 1\n",
      "interpreted -> 1\n",
      "interrupted -> 2\n",
      "intoxicated -> 1\n",
      "intrusively -> 1\n",
      "irregularly -> 1\n",
      "irresistible -> 1\n",
      "latticework -> 1\n",
      "lighthouses -> 4\n",
      "maliciously -> 1\n",
      "massachusetts -> 2\n",
      "merrymakers -> 1\n",
      "mischievously -> 1\n",
      "misunderstood -> 1\n",
      "monasteries -> 2\n",
      "mysteriously -> 1\n",
      "naughtiness -> 1\n",
      "necessaries -> 1\n",
      "neighborhood -> 3\n",
      "netherlands -> 1\n",
      "nevertheless -> 1\n",
      "opportunities -> 1\n",
      "opportunity -> 1\n",
      "overbalancing -> 1\n",
      "overflowing -> 1\n",
      "overpowering -> 1\n",
      "overwhelmed -> 1\n",
      "overwhelming -> 1\n",
      "particularly -> 1\n",
      "peacemakers -> 1\n",
      "performances -> 1\n",
      "perseverance -> 3\n",
      "philadelphia -> 1\n",
      "possessions -> 1\n",
      "possibility -> 1\n",
      "precipitating -> 1\n",
      "preparation -> 1\n",
      "professorship -> 3\n",
      "quarrelsome -> 1\n",
      "reappearance -> 1\n",
      "recognition -> 1\n",
      "recollected -> 1\n",
      "recollection -> 3\n",
      "recommenced -> 1\n",
      "recommendation -> 1\n",
      "refreshment -> 1\n",
      "reluctantly -> 1\n",
      "remembrance -> 1\n",
      "remorselessly -> 1\n",
      "representation -> 2\n",
      "reproachful -> 1\n",
      "reproachfully -> 1\n",
      "resemblance -> 2\n",
      "respectable -> 1\n",
      "righteousness -> 2\n",
      "satisfaction -> 3\n",
      "satisfactory -> 1\n",
      "schoolbooks -> 1\n",
      "schoolhouse -> 7\n",
      "schoolmaster -> 9\n",
      "schoolmates -> 3\n",
      "selfishness -> 3\n",
      "seriousness -> 1\n",
      "shipwrecked -> 1\n",
      "southampton -> 1\n",
      "spatterdashes -> 1\n",
      "sprightliness -> 1\n",
      "straightforward -> 1\n",
      "strawberries -> 2\n",
      "subtracting -> 1\n",
      "suffocating -> 1\n",
      "superiority -> 1\n",
      "surrounding -> 1\n",
      "switzerland -> 1\n",
      "sympathized -> 1\n",
      "sympathizing -> 1\n",
      "thankfulness -> 1\n",
      "theatricals -> 1\n",
      "thermometer -> 1\n",
      "thoroughness -> 1\n",
      "thoughtless -> 3\n",
      "thoughtlessness -> 2\n",
      "transcriber -> 2\n",
      "translucent -> 1\n",
      "troublesome -> 1\n",
      "truthfulness -> 1\n",
      "twitterings -> 1\n",
      "unaccustomed -> 1\n",
      "uncertainty -> 1\n",
      "unconscious -> 3\n",
      "unconsciously -> 2\n",
      "undertaking -> 2\n",
      "undisturbed -> 1\n",
      "unexpectedly -> 1\n",
      "unfortunate -> 1\n",
      "universally -> 1\n",
      "unmanageable -> 1\n",
      "unquestionably -> 1\n",
      "unrestrained -> 1\n",
      "unsuspected -> 1\n",
      "uprightness -> 1\n",
      "whitewashed -> 1\n",
      "wintergreens -> 1\n"
     ]
    }
   ],
   "source": [
    "fdist_grade4 = nltk.FreqDist(text_grade4_tokenize)\n",
    "\n",
    "grade4_complex_words = []\n",
    "for word in sorted(fdist_grade4):\n",
    "    if(len(word) > 10):\n",
    "        print(word, '->', fdist_grade4[word], end='\\n')\n",
    "        grade4_complex_words.append(word)\n",
    "        \n",
    "results_df.loc[1, 'Complex_Wrd_Cnt'] = len(grade4_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandonment -> 1\n",
      "abbreviation -> 1\n",
      "abstractedly -> 1\n",
      "accidentally -> 1\n",
      "accountableness -> 1\n",
      "accumulated -> 1\n",
      "achievement -> 1\n",
      "achievements -> 1\n",
      "acknowledge -> 2\n",
      "acknowledged -> 2\n",
      "acknowledgment -> 1\n",
      "acquaintance -> 3\n",
      "administered -> 1\n",
      "administering -> 1\n",
      "advancement -> 1\n",
      "adventurous -> 1\n",
      "affectionate -> 2\n",
      "aggravating -> 2\n",
      "agricultural -> 1\n",
      "agriculture -> 1\n",
      "alleghanies -> 1\n",
      "ambrosianae -> 1\n",
      "annihilates -> 1\n",
      "anonymously -> 1\n",
      "antediluvian -> 1\n",
      "anticipation -> 1\n",
      "antislavery -> 1\n",
      "appealingly -> 1\n",
      "application -> 3\n",
      "applications -> 1\n",
      "appointment -> 2\n",
      "appreciated -> 1\n",
      "apprehension -> 2\n",
      "apprenticed -> 3\n",
      "apprenticeship -> 1\n",
      "approaching -> 3\n",
      "architecture -> 2\n",
      "arrangements -> 1\n",
      "articulation -> 2\n",
      "artlessness -> 1\n",
      "ascertained -> 3\n",
      "assiduities -> 1\n",
      "astonishing -> 2\n",
      "astonishingly -> 1\n",
      "astonishment -> 1\n",
      "attainments -> 1\n",
      "attentively -> 1\n",
      "attractions -> 1\n",
      "authenticity -> 1\n",
      "authorizing -> 1\n",
      "backwardness -> 1\n",
      "backwoodsman -> 1\n",
      "ballyshannon -> 1\n",
      "barbarously -> 1\n",
      "battlefield -> 1\n",
      "beautifully -> 1\n",
      "bedchambers -> 1\n",
      "benefactress -> 2\n",
      "benevolence -> 6\n",
      "berkhamstead -> 1\n",
      "blackberries -> 1\n",
      "bladensburg -> 1\n",
      "bolingbroke -> 1\n",
      "bollingbroke -> 1\n",
      "bountifully -> 1\n",
      "bracehridge -> 1\n",
      "brandishing -> 1\n",
      "brightening -> 2\n",
      "calculating -> 1\n",
      "candlelight -> 1\n",
      "caravansary -> 1\n",
      "caricatured -> 1\n",
      "carpentering -> 1\n",
      "celebrities -> 1\n",
      "certificate -> 5\n",
      "challenging -> 1\n",
      "chamberlain -> 1\n",
      "characteristic -> 1\n",
      "characterized -> 1\n",
      "charlestown -> 1\n",
      "charterhouse -> 1\n",
      "chestnutting -> 1\n",
      "christopher -> 3\n",
      "circumference -> 1\n",
      "circumstance -> 1\n",
      "circumstances -> 7\n",
      "circumvolutions -> 1\n",
      "civilization -> 1\n",
      "clamorously -> 1\n",
      "collections -> 1\n",
      "combination -> 1\n",
      "comfortable -> 1\n",
      "comfortless -> 1\n",
      "commemorates -> 1\n",
      "commencement -> 1\n",
      "commissioned -> 1\n",
      "commonwealth -> 1\n",
      "communicate -> 1\n",
      "communicated -> 1\n",
      "communicating -> 1\n",
      "compactness -> 1\n",
      "comparatively -> 1\n",
      "complication -> 1\n",
      "compliments -> 2\n",
      "composition -> 2\n",
      "compositions -> 1\n",
      "comprehends -> 1\n",
      "comprehension -> 3\n",
      "concealment -> 2\n",
      "confederate -> 3\n",
      "confidently -> 1\n",
      "confidingly -> 1\n",
      "confinement -> 1\n",
      "conflagration -> 2\n",
      "confounding -> 1\n",
      "congratulating -> 1\n",
      "congratulations -> 1\n",
      "congregated -> 1\n",
      "congregation -> 3\n",
      "congregational -> 1\n",
      "conjectured -> 1\n",
      "conjunction -> 1\n",
      "connecticut -> 1\n",
      "consciousness -> 2\n",
      "consecrated -> 1\n",
      "consequence -> 3\n",
      "consequences -> 3\n",
      "consequently -> 1\n",
      "considerable -> 2\n",
      "considerably -> 2\n",
      "considerately -> 1\n",
      "consideration -> 1\n",
      "consternation -> 3\n",
      "constructed -> 1\n",
      "construction -> 3\n",
      "constructor -> 1\n",
      "consultation -> 1\n",
      "consummation -> 1\n",
      "contemplated -> 1\n",
      "contemplating -> 1\n",
      "contemplation -> 2\n",
      "contemptible -> 2\n",
      "continental -> 3\n",
      "continually -> 7\n",
      "continuance -> 1\n",
      "contracting -> 1\n",
      "contraction -> 1\n",
      "contradicted -> 1\n",
      "contributed -> 8\n",
      "contributing -> 3\n",
      "contributions -> 1\n",
      "contributor -> 5\n",
      "contrivance -> 1\n",
      "controversy -> 1\n",
      "conversation -> 1\n",
      "conversations -> 1\n",
      "cooperstown -> 1\n",
      "correctness -> 2\n",
      "correspondent -> 1\n",
      "corresponding -> 1\n",
      "countenance -> 13\n",
      "countenances -> 2\n",
      "countinghouse -> 1\n",
      "cultivating -> 1\n",
      "cultivation -> 4\n",
      "curiosities -> 1\n",
      "customhouse -> 2\n",
      "declaration -> 9\n",
      "deformities -> 1\n",
      "deliberately -> 3\n",
      "deliberating -> 1\n",
      "delineating -> 1\n",
      "delineation -> 2\n",
      "deliverance -> 1\n",
      "demoniacally -> 1\n",
      "demonstration -> 1\n",
      "demonstrations -> 1\n",
      "denomination -> 1\n",
      "depressions -> 1\n",
      "derwentwater -> 1\n",
      "descendants -> 1\n",
      "description -> 2\n",
      "descriptions -> 2\n",
      "descriptive -> 3\n",
      "desperately -> 1\n",
      "desperation -> 1\n",
      "despondency -> 1\n",
      "destination -> 2\n",
      "destruction -> 8\n",
      "destructive -> 3\n",
      "determination -> 4\n",
      "dexterously -> 1\n",
      "differently -> 1\n",
      "diplomatist -> 1\n",
      "disadvantageous -> 1\n",
      "disaffected -> 1\n",
      "disagreeable -> 1\n",
      "disagreeably -> 1\n",
      "disappeared -> 4\n",
      "disappointed -> 5\n",
      "disappointment -> 2\n",
      "disappointments -> 1\n",
      "disciplined -> 2\n",
      "disconcerted -> 1\n",
      "disconnected -> 1\n",
      "discontented -> 1\n",
      "discontinued -> 1\n",
      "discouraged -> 3\n",
      "discouragement -> 8\n",
      "discourager -> 1\n",
      "discovering -> 1\n",
      "disentangled -> 1\n",
      "disgraceful -> 1\n",
      "disobedience -> 1\n",
      "disobedient -> 1\n",
      "displeasing -> 1\n",
      "displeasure -> 1\n",
      "disposition -> 11\n",
      "disrespectfully -> 1\n",
      "disseminating -> 1\n",
      "dissertation -> 1\n",
      "distinction -> 7\n",
      "distinctive -> 1\n",
      "distinguish -> 2\n",
      "distinguishable -> 1\n",
      "distinguished -> 8\n",
      "distinguishing -> 1\n",
      "distressing -> 1\n",
      "disturbance -> 2\n",
      "diversified -> 1\n",
      "edgeworthtown -> 1\n",
      "editorially -> 1\n",
      "effectually -> 1\n",
      "ejaculations -> 1\n",
      "emancipation -> 1\n",
      "embarrassment -> 1\n",
      "emphasizing -> 1\n",
      "emphatically -> 1\n",
      "employments -> 1\n",
      "enchantment -> 1\n",
      "encouragement -> 2\n",
      "encroachments -> 1\n",
      "endeavoring -> 1\n",
      "enlightened -> 1\n",
      "entanglement -> 1\n",
      "enterprises -> 1\n",
      "entertainment -> 2\n",
      "established -> 7\n",
      "everlasting -> 3\n",
      "examination -> 3\n",
      "exceedingly -> 3\n",
      "excellencies -> 1\n",
      "exclamations -> 1\n",
      "expectation -> 1\n",
      "expectations -> 1\n",
      "expeditions -> 1\n",
      "experienced -> 2\n",
      "experimental -> 1\n",
      "experiments -> 3\n",
      "explanation -> 2\n",
      "expostulated -> 1\n",
      "expressions -> 1\n",
      "extensively -> 3\n",
      "extraordinary -> 3\n",
      "extravagance -> 1\n",
      "fashionable -> 3\n",
      "figuratively -> 2\n",
      "floundering -> 1\n",
      "flourishing -> 2\n",
      "forefathers -> 2\n",
      "foundations -> 1\n",
      "frantically -> 1\n",
      "frequenting -> 2\n",
      "gastronomic -> 1\n",
      "generations -> 2\n",
      "germinating -> 1\n",
      "glimmerings -> 1\n",
      "gloucestershire -> 1\n",
      "gracefulness -> 1\n",
      "grandfather -> 4\n",
      "grandmothers -> 1\n",
      "gratification -> 2\n",
      "gratifications -> 2\n",
      "gravestones -> 1\n",
      "handkerchief -> 1\n",
      "handkerchiefs -> 1\n",
      "handwriting -> 1\n",
      "hardinsburgh -> 1\n",
      "hearthstone -> 2\n",
      "helplessness -> 1\n",
      "hertfordshire -> 1\n",
      "hesitatingly -> 1\n",
      "highlanders -> 3\n",
      "honeysuckle -> 1\n",
      "hopelessness -> 1\n",
      "hospitality -> 3\n",
      "hostilities -> 1\n",
      "ignominious -> 2\n",
      "ignominiously -> 1\n",
      "illuminations -> 1\n",
      "illustrated -> 2\n",
      "illustrious -> 1\n",
      "imagination -> 2\n",
      "imaginative -> 2\n",
      "immediately -> 9\n",
      "impartially -> 2\n",
      "impatiently -> 2\n",
      "impertinent -> 1\n",
      "impracticable -> 1\n",
      "imprecations -> 2\n",
      "impregnable -> 1\n",
      "imprisonment -> 2\n",
      "improvement -> 4\n",
      "improvements -> 2\n",
      "incessantly -> 1\n",
      "inclination -> 3\n",
      "incompatible -> 2\n",
      "inconceivable -> 1\n",
      "inconsiderable -> 1\n",
      "inconstancy -> 1\n",
      "independence -> 12\n",
      "independent -> 1\n",
      "indifference -> 1\n",
      "indifferent -> 1\n",
      "indignation -> 1\n",
      "indiscriminately -> 1\n",
      "indispensable -> 1\n",
      "individuals -> 1\n",
      "indomitable -> 1\n",
      "industrious -> 2\n",
      "industriously -> 1\n",
      "ineffectual -> 1\n",
      "information -> 2\n",
      "inhabitants -> 5\n",
      "inheritance -> 1\n",
      "innumerable -> 2\n",
      "inscription -> 1\n",
      "insignificant -> 1\n",
      "inspiration -> 1\n",
      "installment -> 1\n",
      "instantaneous -> 1\n",
      "instinctively -> 2\n",
      "institution -> 1\n",
      "instruction -> 5\n",
      "instrumental -> 1\n",
      "instruments -> 1\n",
      "intellectual -> 4\n",
      "intellectually -> 1\n",
      "intelligence -> 2\n",
      "interesting -> 2\n",
      "interference -> 1\n",
      "interrupted -> 3\n",
      "intolerable -> 1\n",
      "intolerably -> 1\n",
      "intoxication -> 1\n",
      "introduction -> 1\n",
      "investigate -> 1\n",
      "investigation -> 1\n",
      "invigorated -> 1\n",
      "irreligious -> 1\n",
      "irrepressible -> 1\n",
      "irresistibly -> 1\n",
      "knickerbocker -> 2\n",
      "laughingstock -> 2\n",
      "legislature -> 1\n",
      "lengthening -> 1\n",
      "letterpress -> 2\n",
      "lincolnshire -> 3\n",
      "magistrates -> 1\n",
      "magnanimous -> 1\n",
      "magnificence -> 3\n",
      "magnificent -> 3\n",
      "magnificently -> 1\n",
      "malediction -> 1\n",
      "maliciously -> 1\n",
      "manufacture -> 1\n",
      "manufactured -> 1\n",
      "marlborough -> 1\n",
      "massachusetts -> 3\n",
      "masterpieces -> 1\n",
      "mathematical -> 1\n",
      "mealasabhal -> 2\n",
      "melodiously -> 1\n",
      "ministerial -> 1\n",
      "ministering -> 1\n",
      "miraculously -> 2\n",
      "miscalculation -> 1\n",
      "miscellaneous -> 1\n",
      "mischievous -> 1\n",
      "misfortunes -> 2\n",
      "monochromist -> 1\n",
      "mortifications -> 1\n",
      "mountainous -> 1\n",
      "multiplying -> 1\n",
      "municipality -> 1\n",
      "mythological -> 1\n",
      "necessaries -> 1\n",
      "neighborhood -> 6\n",
      "neighboring -> 2\n",
      "nevertheless -> 2\n",
      "nightingale -> 7\n",
      "northernmost -> 1\n",
      "notwithstanding -> 1\n",
      "obligations -> 1\n",
      "observation -> 5\n",
      "obstinately -> 1\n",
      "occasionally -> 5\n",
      "occupations -> 1\n",
      "occurrences -> 1\n",
      "omnipotence -> 1\n",
      "opportunities -> 4\n",
      "opportunity -> 6\n",
      "originality -> 1\n",
      "ornithologist -> 2\n",
      "ornithology -> 1\n",
      "otterbourne -> 1\n",
      "outstretched -> 1\n",
      "overbearing -> 1\n",
      "overburdened -> 1\n",
      "overgarment -> 1\n",
      "overlooking -> 2\n",
      "overmastering -> 1\n",
      "overpowered -> 1\n",
      "overwhelmed -> 1\n",
      "overwhelming -> 1\n",
      "palpitating -> 1\n",
      "palpitation -> 1\n",
      "pamphleteer -> 1\n",
      "particularly -> 1\n",
      "partnership -> 1\n",
      "passionately -> 1\n",
      "patriarchal -> 2\n",
      "patrimonial -> 1\n",
      "penetrating -> 1\n",
      "pennsylvania -> 4\n",
      "pennsylvanian -> 1\n",
      "perambulate -> 1\n",
      "performance -> 1\n",
      "periodicals -> 8\n",
      "permanently -> 1\n",
      "perpendicularly -> 1\n",
      "perpetrator -> 1\n",
      "perpetually -> 2\n",
      "persecution -> 1\n",
      "philadelphia -> 8\n",
      "philosopher -> 2\n",
      "philosophers -> 1\n",
      "philosophical -> 2\n",
      "picturesque -> 2\n",
      "pincushions -> 1\n",
      "portmanteau -> 2\n",
      "portmanteaus -> 3\n",
      "possessions -> 2\n",
      "precipitancy -> 2\n",
      "precipitated -> 1\n",
      "preconcerted -> 1\n",
      "predecessor -> 1\n",
      "predestinates -> 1\n",
      "predestined -> 1\n",
      "predicament -> 1\n",
      "predominate -> 1\n",
      "premonitory -> 1\n",
      "preparation -> 1\n",
      "preparations -> 3\n",
      "pretentious -> 1\n",
      "principally -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proceedings -> 1\n",
      "productions -> 9\n",
      "professorship -> 2\n",
      "promiscuous -> 1\n",
      "promulgating -> 1\n",
      "pronunciation -> 1\n",
      "propagation -> 2\n",
      "propensities -> 1\n",
      "prophesying -> 1\n",
      "proprieties -> 1\n",
      "prosecution -> 1\n",
      "prospective -> 1\n",
      "provocation -> 1\n",
      "publication -> 7\n",
      "rattlesnake -> 2\n",
      "reappointed -> 1\n",
      "recognition -> 6\n",
      "recollection -> 1\n",
      "recollections -> 2\n",
      "reconciliation -> 1\n",
      "reenforcements -> 1\n",
      "reformation -> 2\n",
      "refreshment -> 1\n",
      "regulations -> 2\n",
      "relinquished -> 1\n",
      "reluctantly -> 1\n",
      "remembering -> 1\n",
      "remembrance -> 3\n",
      "remittances -> 1\n",
      "remonstrate -> 1\n",
      "representatives -> 1\n",
      "represented -> 4\n",
      "reprimanded -> 1\n",
      "republished -> 2\n",
      "resemblance -> 1\n",
      "resignation -> 2\n",
      "respectfully -> 1\n",
      "responsibilitie -> 1\n",
      "restoration -> 1\n",
      "retaliating -> 1\n",
      "retributory -> 1\n",
      "reverberated -> 1\n",
      "reverberating -> 2\n",
      "reverential -> 1\n",
      "revolutionary -> 1\n",
      "righteousness -> 2\n",
      "roundabouts -> 1\n",
      "satisfaction -> 5\n",
      "saunterings -> 1\n",
      "scholarship -> 1\n",
      "schoolfellow -> 1\n",
      "schoolfellows -> 1\n",
      "schoolhouse -> 2\n",
      "schoolmaster -> 3\n",
      "scrupulously -> 2\n",
      "secretaries -> 2\n",
      "selfishness -> 1\n",
      "sensibility -> 2\n",
      "sentimental -> 1\n",
      "settlements -> 1\n",
      "shakespeare -> 7\n",
      "shamelessly -> 1\n",
      "significant -> 1\n",
      "significantly -> 1\n",
      "signification -> 1\n",
      "simultaneous -> 2\n",
      "simultaneously -> 1\n",
      "slaughtered -> 1\n",
      "sovereignty -> 1\n",
      "speechifying -> 1\n",
      "springfield -> 1\n",
      "staffordshire -> 1\n",
      "steadfastly -> 2\n",
      "straightway -> 1\n",
      "strengtheneth -> 1\n",
      "stubbornness -> 1\n",
      "sublieutenant -> 1\n",
      "submissively -> 1\n",
      "subordinate -> 1\n",
      "subsequently -> 2\n",
      "subsistence -> 1\n",
      "substantive -> 1\n",
      "successfully -> 1\n",
      "sufficiency -> 1\n",
      "sufficiently -> 1\n",
      "suitableness -> 1\n",
      "superannuated -> 1\n",
      "superfluities -> 1\n",
      "supernatural -> 2\n",
      "surrendered -> 1\n",
      "surrendering -> 2\n",
      "surrounding -> 3\n",
      "switzerland -> 5\n",
      "sympathetic -> 1\n",
      "tantalizing -> 1\n",
      "telegraphed -> 2\n",
      "temptations -> 1\n",
      "termination -> 1\n",
      "thanatopsis -> 1\n",
      "thanksgiving -> 3\n",
      "thoughtfulness -> 1\n",
      "thoughtless -> 1\n",
      "threatening -> 3\n",
      "thunderstorm -> 1\n",
      "titillation -> 1\n",
      "tranquillity -> 1\n",
      "transcriber -> 2\n",
      "transferring -> 1\n",
      "transformed -> 1\n",
      "transgression -> 1\n",
      "translations -> 9\n",
      "transplanted -> 1\n",
      "transportation -> 2\n",
      "treacherously -> 1\n",
      "tremendously -> 1\n",
      "tremulously -> 1\n",
      "twelvemonth -> 1\n",
      "unadulterated -> 1\n",
      "unblenching -> 1\n",
      "uncertainty -> 3\n",
      "unchangeable -> 1\n",
      "uncomfortable -> 1\n",
      "uncompelled -> 1\n",
      "unconditional -> 1\n",
      "unconscious -> 1\n",
      "unconsciously -> 1\n",
      "uncontrollable -> 1\n",
      "uncurtained -> 1\n",
      "underground -> 1\n",
      "understanding -> 3\n",
      "undertakings -> 1\n",
      "undistinguished -> 1\n",
      "undisturbed -> 1\n",
      "undoubtedly -> 1\n",
      "undulations -> 1\n",
      "unexpectedly -> 2\n",
      "unfortunate -> 2\n",
      "unfortunately -> 1\n",
      "unfrequently -> 2\n",
      "ungentlemanly -> 1\n",
      "unhesitatingly -> 1\n",
      "unimportant -> 1\n",
      "uninterrupted -> 1\n",
      "universally -> 1\n",
      "universities -> 2\n",
      "unnecessarily -> 1\n",
      "unobstructed -> 1\n",
      "unobtrusive -> 1\n",
      "unobtrusively -> 1\n",
      "unpracticed -> 1\n",
      "unprecedented -> 1\n",
      "unprotected -> 1\n",
      "unqualified -> 1\n",
      "unreasoning -> 1\n",
      "unrepentant -> 1\n",
      "unutterable -> 2\n",
      "uprightness -> 1\n",
      "valedictorian -> 1\n",
      "versification -> 2\n",
      "warehouseman -> 1\n",
      "washerwoman -> 1\n",
      "washingtons -> 1\n",
      "westminster -> 3\n",
      "westmoreland -> 1\n",
      "wintergreens -> 1\n",
      "wonderfully -> 2\n",
      "wretchedness -> 1\n",
      "yesternight -> 1\n"
     ]
    }
   ],
   "source": [
    "fdist_grade5 = nltk.FreqDist(text_grade5_tokenize)\n",
    "\n",
    "grade5_complex_words = []\n",
    "for word in sorted(fdist_grade5):\n",
    "    if(len(word) > 10):\n",
    "        print(word, '->', fdist_grade5[word], end='\\n')\n",
    "        grade5_complex_words.append(word)\n",
    "        \n",
    "results_df.loc[2, 'Complex_Wrd_Cnt'] = len(grade5_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_complex = n_vocab_size(grade3_complex_words,\n",
    "                          grade4_complex_words,\n",
    "                          grade5_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df['Complex_Wrd_Cnt_Scaled'] = vocab_size_complex[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocabulary_Count</th>\n",
       "      <th>Vocab_Cnt_Scaled</th>\n",
       "      <th>Complex_Wrd_Cnt</th>\n",
       "      <th>Complex_Wrd_Cnt_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>2974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "      <td>6316</td>\n",
       "      <td>0.476612</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.363934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>9986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>634.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score Vocabulary_Count  Vocab_Cnt_Scaled  \\\n",
       "0     Grade 3                 0.246825             2974          0.000000   \n",
       "1     Grade 4                 0.229714             6316          0.476612   \n",
       "2     Grade 5                 0.229137             9986          1.000000   \n",
       "\n",
       "   Complex_Wrd_Cnt  Complex_Wrd_Cnt_Scaled  \n",
       "0             24.0                0.000000  \n",
       "1            246.0                0.363934  \n",
       "2            634.0                1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the long-word vocabulary size (scaled), similar to the vocabulary size (scaled), increases with grade level. We see that both our scaled scores add more value than just the lexical diversity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Complexity Score\n",
    "\n",
    "- Create a new text difficulty score\n",
    "- Simple mean between lexical diversity, scaled vocabulary size, and scaled long-word vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['New_Complexity_Score'] = (results_df.Lexical_Diversity_Score + \n",
    "                                      results_df.Vocab_Cnt_Scaled + \n",
    "                                      results_df.Complex_Wrd_Cnt_Scaled) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_Level</th>\n",
       "      <th>Lexical_Diversity_Score</th>\n",
       "      <th>Vocab_Cnt_Scaled</th>\n",
       "      <th>Complex_Wrd_Cnt_Scaled</th>\n",
       "      <th>New_Complexity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Grade 3</td>\n",
       "      <td>0.246825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grade 4</td>\n",
       "      <td>0.229714</td>\n",
       "      <td>0.476612</td>\n",
       "      <td>0.363934</td>\n",
       "      <td>0.356753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grade 5</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grade_Level  Lexical_Diversity_Score  Vocab_Cnt_Scaled  \\\n",
       "0     Grade 3                 0.246825          0.000000   \n",
       "1     Grade 4                 0.229714          0.476612   \n",
       "2     Grade 5                 0.229137          1.000000   \n",
       "\n",
       "   Complex_Wrd_Cnt_Scaled  New_Complexity_Score  \n",
       "0                0.000000              0.082275  \n",
       "1                0.363934              0.356753  \n",
       "2                1.000000              0.743046  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.iloc[:, [0,1,3,5,6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In Unit 1 Homework, we looked at the lexical diversity and the vocabulary size. We learnt that while the lexical diversity  score may not provide the right picture, the vocabulary size adds additional information on the complexity of the text.\n",
    "\n",
    "In this homework, we scaled the vocabulary size to a value between 0 to 1. We also computed a list of long-words (more than 10 characters) in the text. Both the trend of scaled vocabulary size and the scaled long-word vocabulary were on expected lines. \n",
    "\n",
    "Finally, we created a new complexity score with the lexical diversity score, scaled vocabulary size score, and the scaled long-word vocabulary score having equal weights. As expected, the new complexity score increased with an increase in grade level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://www.nltk.org/book/ch02.html\n",
    "- https://www.nltk.org/book/ch03.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "- https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
